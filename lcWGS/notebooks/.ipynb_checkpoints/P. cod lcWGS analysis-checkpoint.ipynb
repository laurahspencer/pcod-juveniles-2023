{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50faf7a0",
   "metadata": {},
   "source": [
    "Use the [AFSC's lcWGS analysis pipeline](https://github.com/AFSC-Genetics/lcWGS-pipeline/tree/main) to analyze lcWGS data from juvenile Pacific cod which were experimentally exposed to varying temperatures. The purpose of this data/analysis is to\n",
    "\n",
    "  1) identify from which population(s) the juvenile cod came,   \n",
    "  2) characterize parentage/sibship to help select samples for other analyses (RNASeq, methylation, and lipid analyses).  \n",
    "\n",
    "NOTE: because only a couple fish died during the experiment, this analysis will NOT assess genetic composition differences among treatments (i.e. no selection could occur during experiment) \n",
    "\n",
    "### Get data. \n",
    "\n",
    "The Roberts Lab archived the data on their Nightengales server. I copied it to the directory **/share/afsc/pcod-lcwgs-2023/raw-data** on Sedna using \n",
    "`wget -r -np --no-check-certificate -P raw-data/ https://owl.fish.washington.edu/nightingales/G_macrocephalus/H202SC23041287/01.RawData/`\n",
    "\n",
    "Since I copied the files over via wget (not rsync) I need to make sure they transferred correctly by comparing checksums. \n",
    "\n",
    "```\n",
    "IN=/share/afsc/pcod-lcwgs-2023/raw-data\n",
    "FOLDERS=$(ls -1d ${IN}/G* | awk -F \"/\" '{ print $NF }')\n",
    "for folder in $FOLDERS\n",
    "do\n",
    "cd /share/afsc/pcod-lcwgs-2023/raw-data/${folder}\n",
    "md5sum --check MD5.txt\n",
    "done\n",
    "```\n",
    "\n",
    "The raw-data/ directory contains separate subdirectories for each sample, each containing 2 or 4 gzipped fastq files (.fq.gz). Here are all subdirectories, which are named after sample IDs: \n",
    "\n",
    "```\n",
    "(base) [lspencer@node01 raw-data]$ ls\n",
    " GM1     GM13    GM19   GM5    GM80\n",
    " GM10    GM130   GM2    GM50   GM81\n",
    " GM100   GM131   GM20   GM51   GM82\n",
    " GM101   GM132   GM21   GM52   GM83\n",
    " GM102   GM133   GM22   GM53   GM84\n",
    " GM103   GM134   GM23   GM54   GM85\n",
    " GM104   GM135   GM24   GM55   GM86\n",
    " GM105   GM136   GM25   GM56   GM87\n",
    " GM106   GM137   GM26   GM57   GM88\n",
    " GM107   GM138   GM27   GM58   GM89\n",
    " GM108   GM139   GM28   GM59   GM9\n",
    " GM109   GM14    GM29   GM6    GM90\n",
    " GM11    GM140   GM3    GM60   GM91\n",
    " GM110   GM141   GM30   GM61   GM92\n",
    " GM111   GM142   GM31   GM62   GM93\n",
    " GM112   GM143   GM32   GM63   GM94\n",
    " GM113   GM144   GM33   GM64   GM95\n",
    " GM114   GM145   GM34   GM65   GM96\n",
    " GM115   GM146   GM35   GM66   GM97\n",
    " GM116   GM147   GM36   GM67   GM98\n",
    " GM117   GM148   GM37   GM68   GM99\n",
    " GM118   GM149   GM38   GM69   index.html\n",
    " GM119   GM15    GM39   GM7   'index.html?C=D;O=A'\n",
    " GM12    GM150   GM4    GM70  'index.html?C=D;O=D'\n",
    " GM120   GM151   GM40   GM71  'index.html?C=M;O=A'\n",
    " GM121   GM152   GM41   GM72  'index.html?C=M;O=D'\n",
    " GM122   GM153   GM42   GM73  'index.html?C=N;O=A'\n",
    " GM123   GM154   GM43   GM74  'index.html?C=N;O=D'\n",
    " GM124   GM155   GM44   GM75  'index.html?C=S;O=A'\n",
    " GM125   GM156   GM45   GM76  'index.html?C=S;O=D'\n",
    " GM126   GM16    GM46   GM77\n",
    " GM127   GM160   GM47   GM78\n",
    " GM128   GM17    GM48   GM79\n",
    " GM129   GM18    GM49   GM8\n",
    "```\n",
    "\n",
    "Here are the contents of a couple sample subdirectories:\n",
    "```\n",
    "(base) [lspencer@node01 raw-data]$ ls GM10/\n",
    " GM10_CKDN230011848-1A_H5NNGDSX7_L1_1.fq.gz\n",
    " GM10_CKDN230011848-1A_H5NNGDSX7_L1_2.fq.gz\n",
    " index.html\n",
    "'index.html?C=D;O=A'\n",
    "'index.html?C=D;O=D'\n",
    "'index.html?C=M;O=A'\n",
    "'index.html?C=M;O=D'\n",
    "'index.html?C=N;O=A'\n",
    "'index.html?C=N;O=D'\n",
    "'index.html?C=S;O=A'\n",
    "'index.html?C=S;O=D'\n",
    " MD5.txt\n",
    "(base) [lspencer@node01 raw-data]$ ls GM1/\n",
    " GM1_CKDN230011839-1A_H5NNGDSX7_L1_1.fq.gz\n",
    " GM1_CKDN230011839-1A_H5NNGDSX7_L1_2.fq.gz\n",
    " GM1_CKDN230011839-1A_H733MDSX7_L1_1.fq.gz\n",
    " GM1_CKDN230011839-1A_H733MDSX7_L1_2.fq.gz\n",
    " index.html\n",
    "'index.html?C=D;O=A'\n",
    "'index.html?C=D;O=D'\n",
    "'index.html?C=M;O=A'\n",
    "'index.html?C=M;O=D'\n",
    "'index.html?C=N;O=A'\n",
    "'index.html?C=N;O=D'\n",
    "'index.html?C=S;O=A'\n",
    "'index.html?C=S;O=D'\n",
    " MD5.txt\n",
    "```\n",
    "\n",
    "### Concatenate by sample\n",
    "\n",
    "The data is paired-end, R1 and R2 for each sample, but some of the samples also have two files for each R1 and R2 (e.g. see files for \"GM1\" above). So, I need to concatenate before running my pipeline. I'll also rename the files to only include sample ID. Code for concatenating is saved in the script [concat_fastq_files.sh](), but I ran it interactively on Sedna so will paste it here too. The result if this code is two files for each sample named \"GM{ID}\\_R{1,2}.fastq\", one for each read, saved in the directory /share/afsc/pcod-lcwgs-2023/concat.\n",
    "\n",
    "```\n",
    "IN=/share/afsc/pcod-lcwgs-2023/raw-data\n",
    "OUT=/home/lspencer/pcod-lcwgs-2023/analysis-20230828\n",
    "\n",
    "SAMPLES=$(ls -1d ${IN}/G* | awk -F \"/\" '{ print $NF }')\n",
    "\n",
    "for sample in ${SAMPLES}\n",
    "do\n",
    "  echo ${sample}\n",
    "  zcat ${sample}/${sample}_*_L*_1.fq.gz \\\n",
    "      >> ${OUT}/${sample}_R1.fastq\n",
    "  zcat ${sample}/${sample}_*_L*_2.fq.gz \\\n",
    "      >> ${OUT}/${sample}_R2.fastq\n",
    "done\n",
    "```\n",
    "\n",
    "### Prepare input files for lcWGS pipeline\n",
    "\n",
    "First I cloned lcWGS github to Sedna using `git clone https://github.com/AFSC-Genetics/lcWGS-pipeline.git`, which downloaded the [lcWGS-pipeline repo](https://github.com/AFSC-Genetics/lcWGS-pipeline/tree/main) to Sedna in /home/lspencer/. I then prepared input files: \n",
    "- **lcWGS_config.txt** - created on my pers. computer and transferred to Sedna. Here is what it contains: \n",
    "\n",
    "```\n",
    "# Remember to provide the full paths to files and directories.\n",
    "#The reference genome MUST be a fasta with the \".fa\" file extension.\n",
    "#The working directory should contain the list of fastq files and the gzipped fastq files.\n",
    "# Don't worry about adding modules yourself. The pipeline will check for all needed modules and add them if they are not found.\n",
    "# If you run into trouble, please contact Laura.Timm@noaa.gov\n",
    "\n",
    "What file contains the list of FASTQs that will go into the analysis?   /home/lspencer/pcod-lcwgs-2023/analysis-20230828/pcod-lcWGS_fastqs.txt\n",
    "What file contains adapter sequences for TRIMMOMATIC?   /home/lspencer/pcod-lcwgs-2023/analysis-20230828/novogene-adapters.txt\n",
    "What chromosomes/contigs are to be analyzed in this run?        /home/lspencer/pcod-lcwgs-2023/analysis-20230828/chromosomes.txt\n",
    "What is the name of the FASTA file containing the reference genome?     /home/lspencer/references/pcod/PGA_assembly_hap2.chrom_only.fa\n",
    "What is the path to the working directory?      /home/lspencer/pcod-lcwgs-2023/analysis-20230828/\n",
    "What prefix would you like associated with this run?    pcod-lcWGS\n",
    "Where should failed job notifications be sent?  laura.spencer@noaa.gov\n",
    "```\n",
    "\n",
    "- **Pipeline Python and R scripts** - all saved in the lcWGS-pipeline repo and made executable via `chmod u+x {filename}`. NOTE: I did not add all scripts to my bin. Instead I will run them using their full path.    \n",
    "- **P. cod reference genome, I'm using PGA_assembly_hap2.chrom_only.fa** - Ingrid sent to me via email, and downloaded to my pers. computer then copied to /home/lspencer/references/pcod/, and renamed to change from \".fasta\" to \".fa\" (as specificed by the pipeline README file).  \n",
    "- **chromosomes.txt** - list of chromosomes in P. cod genome, created via `grep \">\" ../references/pcod/PGA_assembly_hap2.chrom_only.fa | tr -d '>' >> chromosome.txt`  \n",
    "- **pcod-lcWGS_fastqs.txt**, a list of .fq files - I created this by moving to the directory with concatenated data (/home/lspencer/pcod-lcwgs-2023/concat) and running ```ls *.fastq | while read file; do newfile=`echo /home/lspencer/pcod-lcwgs-2023/analysis-20230828/$file`; echo $newfile >> /home/lspencer/pcod-lcwgs-2023/analysis-20230828/pcod-lcWGS_fastqs.txt; done```\n",
    "\n",
    "Do these for each session: \n",
    "- Activate virtual environment to access MultiQC via `source /home/lspencer/venv/bin/activate`\n",
    "- Define path variables: \n",
    "```\n",
    "scripts=/home/lspencer/lcWGS-pipeline/  \n",
    "inputs=/home/lspencer/pcod-lcwgs-2023/analysis-20230828/\n",
    "```\n",
    "\n",
    "Now I am ready to run the scripts. \n",
    "\n",
    "### Run Steps 0-3 to generate SLURM scripts \n",
    "\n",
    "#### Step0: Configure\n",
    "```\n",
    "(venv) (base) [lspencer@node01 analysis-20230828]$ ${scripts}lcWGSpipeline_step0-configure.py -c ${inputs}lcWGS_config.txt\n",
    "Step 0 has finished successfully! You will find two new scripts in ./scripts/: /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/PGA_assembly_hap2.chrom_only_bwa-indexSLURM.sh and /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/PGA_assembly_hap2.chrom_only_faiSLURM.sh.\n",
    "Both scripts can run simultaneously with 'sbatch'.\n",
    " However, there is no need to wait for these jobs to finish to move to step 1.\n",
    "To continue on, call step 1 to generate scripts for fastQC and multiQC.\n",
    "Remember to pass the newly-made checkpoint (.ckpt) file with the '-p' flag.\n",
    "```\n",
    "\n",
    "#### Step1: QC\n",
    "```\n",
    "(venv) (base) [lspencer@node01 analysis-20230828]$ ${scripts}lcWGSpipeline_step1-qc.py -p pcod-lcWGS.ckpt\n",
    "Step 1 has finished successfully! You will find two new scripts in ./scripts/: /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-raw_fastqcARRAY.sh and /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-raw_multiqcSLURM.sh.\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-raw_fastqcARRAY.sh must run prior to launching /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-raw_multiqcSLURM.sh.\n",
    " However, there is no need to wait for these jobs to finish to move to step 2.\n",
    "To continue on, call step 2 to generate a script for TRIMMOMATIC.\n",
    "Remember to pass the checkpoint (.ckpt) file with the '-p' flag.\n",
    "```\n",
    "\n",
    "#### Step2: Trim\n",
    "```\n",
    "(venv) (base) [lspencer@node01 analysis-20230828]$ ${scripts}lcWGSpipeline_step2-trim.py -p pcod-lcWGS.ckpt\n",
    "Step 2 has finished successfully! You will find three new scripts in ./scripts/: /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_trimARRAY.sh, /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-trim_fastqcARRAY.sh, and /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS-trim_multiqcSLURM.sh.\n",
    "Each script must run in the above order and each job must finish before submitting the next.\n",
    "While there is no need to wait before running step 3 to generate the alignment script, it is probably wise to wait until the multiQC script has completed and the results have been viewed in a web browser prior to submitting the script written by step 3.\n",
    "Remember to pass the checkpoint (.ckpt) file with the '-p' flag.\n",
    "```\n",
    "\n",
    "#### Step3: Align\n",
    "\n",
    "```\n",
    "(base) [lspencer@sedna analysis-20230828]$ /home/lspencer/lcWGS-pipeline/lcWGSpipeline_step3-align.py -p pcod-lcWGS.ckpt\n",
    "Step 3 has finished successfully! You will find two new scripts in ./scripts/: /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_alignARRAY.sh and /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_depthsARRAY.sh.\n",
    "These scripts must be run in the order given above and both must finish before moving on to step 4.\n",
    "After /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_depthsARRAY.sh has run, download the resulting file: pcod-lcWGS_depths.csv and generate a barchart of mean depths by individual. This will help you determine whether any individuals fall substantially below the average depth (usually, we use a cutoff of 1x).\n",
    "If you identify samples with coverage that is 'too low', add the sample id(s) to a new file, referred to as the 'blacklist' of individuals to be excluded from genotype likelhiood calculations and the final data sets.\n",
    "After generating this blacklist, you can continue to step 4 to write scripts for generating the final data sets.\n",
    "Remember to pass the checkpoint (.ckpt) file with the '-p' flag AND the blacklist file with the '-b' flag.\n",
    "```\n",
    "\n",
    "### Run SLURM scripts from steps 0-3. \n",
    "\n",
    "As per instructions, edited the MultiQC scripts (for raw and trimmed data) using nano to use the correct path to activate my virtual environment (/home/lspencer/venv/bin/activate)\n",
    "\n",
    "Ran these three scripts immediately  \n",
    "`sbatch scripts/PGA_assembly_hap2.chrom_only_bwa-indexSLURM.sh`  \n",
    "`sbatch scripts/PGA_assembly_hap2.chrom_only_faiSLURM.sh`  \n",
    "`sbatch scripts/pcod-lcWGS-raw_fastqcARRAY.sh`  \n",
    "\n",
    "Then this one to run multiqc - `sbatch scripts/pcod-lcWGS-raw_multiqcSLURM.sh` - After it finished I transferred the multiqc files to my local computer to view and renamed to \"_raw\". Raw data looks good! \n",
    "\n",
    "Then ran these scripts, one after the other:  \n",
    "`sbatch scripts/pcod-lcWGS_trimARRAY.sh`  \n",
    "`sbatch scripts/pcod-lcWGS-trim_fastqcARRAY.sh`  \n",
    "`sbatch scripts/pcod-lcWGS-trim_multiqcSLURM.sh`  \n",
    "\n",
    "Transferred the multiqc files to my local computer, renamed to \"_trimmed\". Looks good!\n",
    "\n",
    "Ran the alignment scripts one after the other:   \n",
    "`sbatch pcod-lcWGS_alignARRAY.sh`   \n",
    "`sbatch scripts/pcod-lcWGS_depthsARRAY.sh` - I had to modify this script to include the full path for the python script `/home/lspencer/lcWGS-pipeline/mean_cov_ind.py.`\n",
    "\n",
    "This barplot shows the mean coverage depth for each sample (sorted by depth), color coded by temperature treatment. The lcWGS pipeline states that samples with mean depth <1 should be blacklisted. All samples look good! Interesting that samples with lowest mean coverage are from warmer treatments, and those with highest mean coverage are from the 5C treatment. \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "#### Step4: Get Data\n",
    "\n",
    "Ran the step4-data python script to generate SLURM scripts which calculate genotype likelihoods:\n",
    "\n",
    "```\n",
    "(base) [lspencer@node01 analysis-20230828]$ /home/lspencer/lcWGS-pipeline/lcWGSpipeline_step4-data.py -p pcod-lcWGS.ckpt -b blacklist.txt\n",
    "Step 4 has finished successfully! You will find two new scripts in ./scripts/:\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_globalARRAY.sh calculates genotype likelihoods across all sites on each chromosome (separately).\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_polymorphicARRAY.sh calculates genotype likelihoods across all polymorphic sites on each chromosome (separately).\n",
    "\n",
    "Both scripts can run simultaneously.\n",
    "After they have run, you will have genotype likelihoods (gls) and allele frequencies (maf) for all sites in the genome (global) and putatively variable sites (polymorphic).\n",
    "```\n",
    "\n",
    "Ran those two SLURM scripts produced by `sbatch scripts/pcod-lcWGS_globalARRAY.sh` and `sbatch scripts/pcod-lcWGS_polymorphicARRAY.sh`. \n",
    "\n",
    "#### Step5: Collate\n",
    "\n",
    "Ran the step5-collate script to generate SLURM scripts to concatenate all the chromosomes and index polymorphic sites across the whole genome:\n",
    "\n",
    "```\n",
    "(base) [lspencer@node01 analysis-20230828]$ /home/lspencer/lcWGS-pipeline/lcWGSpipeline_step5-collate.py -p pcod-lcWGS.ckpt\n",
    "Two new scripts have been generated:\n",
    "\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_concatenate_beagles.sh concatenates all the polymorphic beagle files to produce a single file containing genotype likelihoods for all polymorphic sites across the genome.\n",
    "\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_concatenate_mafs.sh concatenates all the polymorphic maf files to produce a single file containing minor allele frequencies for all polymorphic sites across the genome.\n",
    "\n",
    "Congratulations! Data assembly is complete!\n",
    "```\n",
    "\n",
    "Then ran the resulting slurm scripts: `lcWGS_concatenate_beagles.sh` & `sbatch scripts/pcod-lcWGS_concatenate_mafs.sh`\n",
    "\n",
    "#### Step6: PCA & Admixture analysis\n",
    "\n",
    "Ran the python code for PCA and admixture: \n",
    "```\n",
    "(base) [lspencer@node04 analysis-20230828]$ /home/lspencer/lcWGS-pipeline/lcWGSpipeline_pca-admixture.py -p pcod-lcWGS.ckpt -k 10\n",
    "Three scripts have been generated: /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_pcangsdARRAY.sh, /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_wholegenome_pcangsd.sh, and /home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_wholegenome_admixARRAY.sh.\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_pcangsdARRAY.sh will run pca for each chromosome.\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_wholegenome_pcangsd.sh will run pca for the whole genome polymorphic data.\n",
    "/home/lspencer/pcod-lcwgs-2023/analysis-20230828/scripts/pcod-lcWGS_wholegenome_admixARRAY.sh will launch an array of admixture jobs, testing every value of K between 1 and the max k value entered by the user (three replicates will run for each K value).\n",
    "All three scripts can run in parallel.\n",
    "```\n",
    "\n",
    "#### Generate PCA's in R using the pipelines' resulting covariate matrices \n",
    "\n",
    "Using R (script \"lcWGS-analysis.Rmd) I ran PCA's on each chromosomal covariate matrix using the function `prcomp()`. Here are [PCA's from all chromosomes, pcas.pdf](https://github.com/laurahspencer/pcod-juveniles-2023/blob/main/lcWGS/analysis-20230823/pca/pcas.pdf). Here is one interesting one (Chr 12) and the whole genome PCA: \n",
    "\n",
    "![image-4.png](attachment:image-4.png)\n",
    "\n",
    "![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55605f97",
   "metadata": {},
   "source": [
    "## Re-Analysis including reference fish \n",
    "\n",
    "I got the lcWGS pipeline to work using our data! The PCA's indicated no obvious clustering by temperature treatment. BUT we are also very interested in the genetic origin of our wild-caught juveniles. Luckily, Sara Schaal has been characterizing the population structure of P. cod, and has samples with known origin. She had samples from hundreds of fish, so I asked for a subset of those fish for populations of interest. Here's her reply:\n",
    "\n",
    "_Here are lists of samples from either 5 or 10 individuals from 8 populations in my dataset: Hecate Strait, Kodiak, Shumigans, Unimak, Pervenets Canyon, Amchitka Pass, Tanaga Island, and Russia. The original genomewide pca (pcod20230117_genomewide_pca) is what I used to subset some individuals out for you. I replotted the samples using their pca coordinates from this analysis so you see which ones I chose. I avoided any of those intermediate samples from the Bering Sea and only chose samples from our first sequencing run batch. I gave you a list of 5 and 10 per population because I'm not sure how many samples you'll need to pull out differences. It might be interesting to compare using both to see if it changes your interpretation. PCAs can be finicky depending on what samples and how many are in it... I can run a pca with just those samples using my genotype likelihood file that I have and get back to you with how well the 5 or 10 individuals pull apart those 8 populations. I just probably won't be able to get to that until tomorrow or Wednesday. I also attached a version of the ABLG database that has my edits for the population names included under the \"Location1\" column._  \n",
    "\n",
    "So, I reran the lcWGS pipeline with my juvenile P. cod samples AND the subset of samples from known origins. \n",
    "\n",
    "### Prepare Inputs \n",
    "\n",
    "1. Created new analysis folder on Sedna: /home/lspencer/pcod-lcwgs-2023/analysis-20230922/  \n",
    "2. **Moved** all my concatenated data files to new analysis folder  \n",
    "3. Copied over the subset of reference files that Sara provided from Sedna Gold to Sedna: \n",
    "  a. Created a list of _all_ reference **filenames** on Sedna Gold using `ls *.fq.gz > /home/laura.spencer/pcod_lcwgs_reference.txt`, then transferred that over to my personal computer using rsync. \n",
    "  b. In RStudio, imported the ABLGs_10inds_per_pop.txt file that Sara sent over (which contains **sample ID's** for 10 individuals per population of interest) and the pcod_lcwgs_reference.txt file that I created in step 1. Joined the two files and then saved a new .txt file that contained only the **filenames** for the samples of interest, \"pcod_10refs.txt\". Moved pcod_10refs.txt over to Sedna in my new analysis folder. \n",
    "  c. Used `rsync` to copy over the files of interest (fq.gz) from Sedna Gold to Sedna: `rsync --archive --progress --verbose --files-from=pcod_10refs.txt laura.spencer@161.55.97.203://sednagold/Gadus_macrocephalus/novaseq .` (this code was executed from Sedna).  \n",
    "  d. Unzipped all the fq.gz files using `gzip -dv *.fq.gz`  \n",
    "\n",
    "4. Prepared other inputs: Copied chromosomes.txt, adapters.txt, and lcWGS_config.txt from old analysis folder to new analysis folder. \n",
    "  a. Modified adapters.txt to include [adapters on reference data](https://github.com/AFSC-Genetics/lcWGS-pipeline/blob/main/NexteraPE-PE.fa) (This was provided by Sara).  \n",
    "  b. Modifed lcWGS_config.txt with new analysis directory and file names.  \n",
    "  c. Created new file list via: \n",
    " ```\n",
    "ls *.{fastq,fq} | while read file; do newfile=`echo /home/lspencer/pcod-lcwgs-2023/analysis-20230922/$file`; echo $newfile >> /home/lspencer/pcod-lcwgs-2023/analysis-20230922/pcod-lcWGS_fastqs.txt; done\n",
    " ```\n",
    "6. Run pipeline! \n",
    "\n",
    "Activate virtual environment to access MultiQC: \n",
    "`source /home/lspencer/venv/bin/activate`\n",
    "\n",
    "Define path variables:\n",
    "`scripts=/home/lspencer/lcWGS-pipeline/`  \n",
    "`inputs=/home/lspencer/pcod-lcwgs-2023/analysis-20230922/`  \n",
    "\n",
    "Run first three python scripts: \n",
    "```\n",
    "${scripts}lcWGSpipeline_step0-configure.py -c ${inputs}lcWGS_config.txt\n",
    "${scripts}lcWGSpipeline_step2-trim.py -p pcod-lcWGS.ckpt\n",
    "/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step3-align.py -p pcod-lcWGS.ckpt\n",
    "```\n",
    "Then initiated the resulting SLURM scripts (so all ran at the same time):\n",
    "```\n",
    "sbatch scripts/PGA_assembly_hap2.chrom_only_bwa-indexSLURM.sh\n",
    "sbatch scripts/PGA_assembly_hap2.chrom_only_faiSLURM.sh\n",
    "sbatch scripts/pcod-lcWGS-raw_fastqcARRAY.sh\n",
    "```\n",
    "When the SLURM jobs were done, ran the multiqc script - `sbatch scripts/pcod-lcWGS-raw_multiqcSLURM.sh` - After it finished I transferred the multiqc files to my local computer to view and renamed to \"_raw\". There's definitely differences among the data from our experimental animals and the reference data. \n",
    "\n",
    "I then ran these scripts, one after the other, to trim and then look at the trimmed data:\n",
    "```\n",
    "sbatch scripts/pcod-lcWGS_trimARRAY.sh  \n",
    "sbatch scripts/pcod-lcWGS-trim_fastqcARRAY.sh  \n",
    "sbatch scripts/pcod-lcWGS-trim_multiqcSLURM.sh  \n",
    "```\n",
    "\n",
    "Transferred the multiqc html file to my local computer, renamed to \"_trimmed\". Looks good!\n",
    "\n",
    "Ran the alignment scripts one after the other:  \n",
    "`sbatch pcod-lcWGS_alignARRAY.sh`  \n",
    "`sbatch scripts/pcod-lcWGS_depthsARRAY.sh` - I had to modify this script to include the full path for the python script  `/home/lspencer/lcWGS-pipeline/mean_cov_ind.py.`  \n",
    "\n",
    "This barplot shows the mean coverage depth for each sample (sorted by depth), color coded by temperature treatment or sample location. The lcWGS pipeline states that samples with mean depth <1 should be blacklisted. All samples look good! \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Copied the empty blacklist over to the new analysis director, and ran `/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step4-data.py -p pcod-lcWGS.ckpt -b blacklist.txt`, then ran those two SLURM scripts: `sbatch scripts/pcod-lcWGS_globalARRAY.sh` and `sbatch scripts/pcod-lcWGS_polymorphicARRAY.sh`\n",
    "\n",
    "Ran the collate script: `/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step5-collate.py -p pcod-lcWGS.ckpt`  \n",
    "Then ran the resulting slurm scripts: `sbatch scripts/pcod-lcWGS_concatenate_beagles.sh` & `sbatch scripts/pcod-lcWGS_concatenate_mafs.sh`\n",
    "\n",
    "Ran the PCA/Admixture python script: `/home/lspencer/lcWGS-pipeline/lcWGSpipeline_pca-admixture.py -p pcod-lcWGS.ckpt -k 10` then the three resulting scripts in parallel: `pcod-lcWGS_pcangsdARRAY.sh`, `pcod-lcWGS_wholegenome_pcangsd.sh`, and `pcod-lcWGS_wholegenome_admixARRAY.sh`\n",
    "\n",
    "### PCA results using subset of reference fish \n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "![image-3.png](attachment:image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980eb61",
   "metadata": {},
   "source": [
    "# lcWGS Re-Analysis with ALL reference fish, prep for WGSassign \n",
    "\n",
    "I am now tackling this lcWGS analysis with more ammo. First, I will run the AFSC lcWGS pipeline using ALL of Sara's reference fish (N=), second, based on Sara Schaal's suggestion, I will use WGSassign to identify population of origin for our experimental fishes. Here's what Sara had to say: \n",
    "\n",
    "_I would still try with all the loci and use the low-coverage assignment method that Eric Anderson's group put out (paper here). If it is anything like GTseq which I assume it will be, what you would have to do is build a sites file from the baseline dataset (all my samples) and only use those sites with your dataset. Otherwise there will be a ton of sites in one dataset, but not the other which will drive the PCA (hence big batch effect). Then once you only get data from those sites you would filter based on missing data. That would probably need to be played with a bit to see what levels of missing data you can get away with. I had to go fairly high with GTseq to get rid of obvious missing data driving results. A PCA is good for determine how bad the missing data influences results before you do any assignments._ \n",
    "\n",
    "_I think this is your best option for assignment. Low-coverage data is hard to get accurate genotype calls from so I'd be worried about biases in your called genotype data from low-coverage. I really think that WGSassign is what you need._ \n",
    "\n",
    "_I'd be more than happy to try and help you implement this method. Its something I've wanted to do with the baseline for awhile, but just haven't had time. I do have sites for GT-seq and I can share that, but I would still be worried about your results given called genotypes from low-coverage data. I have markers in ZP3 that I purposely designed the panel to include. Let me know what you think and maybe we could buddy up trying to get WGSassign to work for your data. I think it'll be useful for a lot of folks and for me too so I'd be happy to carve out time to work on this with you._\n",
    "\n",
    "\n",
    "## Running AFSC lcWGS pipeline \n",
    "\n",
    "### Prepare Inputs\n",
    "\n",
    "- Created a new analysis directory with two subdirectories, **analysis20240606/reference/** for all lcWGS data from fish of known origin (filtered to remove some fish that can cause issues, as per Sara/Ingrid), and **analysis20240606/experimental/** with lcWGS from our experimental fish of unknown origin.  \n",
    "- Switched to use the NCBI version of the genome, since that's what I'm using for RNASeq and Ingrid is also using that  \n",
    "- Created a new chromosomes_pcod-ncbi.txt file using `grep \">\" /home/lspencer/references/pcod-ncbi/GCF_031168955.1_ASM3116895v1_genomic.fa | tr -d '>' >> chromosomes_pcod-ncbi.txt`\n",
    "- Copied the input files created previously (adapters.txt, blacklist.txt, lcWGS_config.txt) to each subdirectory.  \n",
    "- Modified the lcWGS_config.txt file with appropriate paths.  \n",
    "- rsynced the experimental fish data (all .fq beginning with \"GM\") from old analysis folder to the experimental/ subdirectory (used `rsync` instead of `mv` to ensure data integrity)   \n",
    "- To get the all the reference fish lcWGS data, I transferred them from SednaGold using rsync: \n",
    "\n",
    "```\n",
    "cd /home/lspencer/pcod-lcWGS/analysis20240606/reference/\n",
    "\n",
    "rsync --archive --progress --verbose laura.spencer@161.55.97.203:/sednagold/Gadus_macrocephalus/novaseq/*.fq.gz .\n",
    "rsync --archive --progress --verbose laura.spencer@161.55.97.203:/sednagold/Gadus_macrocephalus/novaseq/20220713_secondSeqRun/*.fq.gz .\n",
    "rsync --archive --progress --verbose laura.spencer@161.55.97.203:/sednagold/Gadus_macrocephalus/novaseq/20221215_seqR3/*.fastq.gz .\n",
    "```\n",
    "\n",
    "For each data set, created file that lists the data: \n",
    "\n",
    "```\n",
    "# Reference data - NOTE: NOT USING THIS BECAUSE IT CONTAINS SOME FISH THAT SARA SUGGESTED TO NOT USE\n",
    "#cd /home/lspencer/pcod-lcWGS/analysis20240606/reference/\n",
    "#ls -d $PWD/ABLG* > pcod-reference-fastqs.txt\n",
    "\n",
    "# ACTUAL list of fish I'm using as references: pcod-reference-fastqs-filetered.txt\n",
    "\n",
    "# Experimental data\n",
    "cd ../experimental/\n",
    "ls -d $PWD/GM* > pcod-experimental-fastqs.txt\n",
    "```\n",
    "\n",
    "### Run pipeline! \n",
    "\n",
    "#### First, run on reference fish \n",
    "\n",
    "Initiate interactiv node on Sedna\n",
    "`srun --pty /bin/bash`\n",
    "\n",
    "Activate virtual environment to access MultiQC: \n",
    "```\n",
    "source /home/lspencer/venv/bin/activate\n",
    "```\n",
    "\n",
    "Define path variables:\n",
    "```\n",
    "scripts=/home/lspencer/lcWGS-pipeline/\n",
    "inputs=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/\n",
    "```\n",
    "\n",
    "Run first four python scripts, which generate slurm scripts needed for fastqc, trimming, and alignment: \n",
    "```\n",
    "${scripts}lcWGSpipeline_step0-configure.py -c ${inputs}lcWGS_config.txt\n",
    "${scripts}lcWGSpipeline_step1-qc.py -p pcod-refs.ckpt\n",
    "${scripts}lcWGSpipeline_step2-trim.py -p pcod-refs.ckpt\n",
    "/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step3-align.py -p pcod-refs.ckpt\n",
    "```\n",
    "\n",
    "As per instructions, edited the MultiQC scripts (for raw and trimmed data) using nano to use the correct path to activate my virtual environment (/home/lspencer/venv/bin/activate)\n",
    "\n",
    "Then initiated these SLURM scripts (they ran at the same time):\n",
    "```\n",
    "sbatch scripts/GCF_031168955.1_ASM3116895v1_genomic_bwa-indexSLURM.sh\n",
    "sbatch scripts/GCF_031168955.1_ASM3116895v1_genomic_faiSLURM.sh\n",
    "```\n",
    "\n",
    "#### Issue encountered due to >1k sample files \n",
    "These jobs are run as job arrays (1 per .fq file), which have a max arrage size of 1000. I have 1,310 .fq files to process so cannot simply run the slurm array scripts. With help from Giles, I broke the fastqARRAY.sh into two separate jobs: \n",
    "\n",
    "**Script #1. scripts/pcod-refs-raw_fastqcARRAY-1.sh, which runs through files 1-1000**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=fqc_array_pcod-refs\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --output=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/job_outfiles/pcod-refs-raw_fastqc_%A-%a.out\n",
    "#SBATCH --mail-type=FAIL\n",
    "#SBATCH --mail-user=laura.spencer@noaa.gov\n",
    "#SBATCH --time=0-03:00:00\n",
    "#SBATCH --array=1-1000%24\n",
    "\n",
    "module unload bio/fastqc/0.11.9\n",
    "module load bio/fastqc/0.11.9\n",
    "\n",
    "JOBS_FILE=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/scripts/pcod-refs-raw_fqcARRAY_input.txt\n",
    "IDS=$(cat ${JOBS_FILE})\n",
    "\n",
    "for sample_line in ${IDS}\n",
    "do\n",
    "        job_index=$(echo ${sample_line} | awk -F \":\" '{print $1}')\n",
    "        fq=$(echo ${sample_line} | awk -F \":\" '{print $2}')\n",
    "        if [[ ${SLURM_ARRAY_TASK_ID} == ${job_index} ]]; then\n",
    "                break\n",
    "        fi\n",
    "done\n",
    "\n",
    "fastqc ${fq} -o /home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/fastqc/raw/\n",
    "```\n",
    "\n",
    "**Script #2. scripts/pcod-refs-raw_fastqcARRAY-2.sh, which runs through files 1001+**\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=fqc_array_pcod-refs\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --output=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/job_outfiles/pcod-refs-raw_fastqc_%A-%a.out\n",
    "#SBATCH --mail-type=FAIL\n",
    "#SBATCH --mail-user=laura.spencer@noaa.gov\n",
    "#SBATCH --time=0-03:00:00\n",
    "#SBATCH --array=1-310%24\n",
    "\n",
    "module unload bio/fastqc/0.11.9\n",
    "module load bio/fastqc/0.11.9\n",
    "\n",
    "JOBS_FILE=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/scripts/pcod-refs-raw_fqcARRAY_input.txt\n",
    "IDS=$(cat ${JOBS_FILE})\n",
    "\n",
    "POSMOD=1000\n",
    "NEW_TASK_ID=$((${SLURM_ARRAY_TASK_ID} + ${POSMOD}))\n",
    "\n",
    "for sample_line in ${IDS}\n",
    "do\n",
    "        job_index=$(echo ${sample_line} | awk -F \":\" '{print $1}')\n",
    "        fq=$(echo ${sample_line} | awk -F \":\" '{print $2}')\n",
    "        if [[ ${NEW_TASK_ID} == ${job_index} ]]; then\n",
    "                break\n",
    "        fi\n",
    "done\n",
    "\n",
    "fastqc ${fq} -o /home/lspencer/pcod-lcwgs-2023/analysis-20240606/reference/fastqc/raw/\n",
    "```\n",
    "\n",
    "When the fastqc SLURM jobs were done, edited the multiqc script to remove the cpus option (deleted `#SBATCH --cpus-per-task=1`) and replaced that to specify using a himem node (`#SBATCH -p himem`). I then ran the script `sbatch scripts/pcod-refs-raw_multiqcSLURM.sh`, after it finished I renamed it to \"_raw\" and transferred the multiqc files to my local computer to view: \"pcod-juveniles-2023\\lcWGS\\analysis-20240606/references/multiqc_report_raw.html\". \n",
    "\n",
    "I then edited the trim script to increase the time (it was just 12hr before) and ran it:\n",
    "```\n",
    "sbatch scripts/pcod-refs_trimARRAY.sh\n",
    "```\n",
    "\n",
    "I then split the pcod-refs-trim_fastqcArray.sh script into two as I did before with the raw data, since the array number exceeded 1,000, ran those, then when they were finished I ran the multiQC script. \n",
    "\n",
    "```\n",
    "sbatch scripts/pcod-refs-trim_fastqcARRAY-1.sh  \n",
    "sbatch scripts/pcod-refs-trim_fastqcARRAY-2.sh  \n",
    "sbatch scripts/pcod-refs-trim_multiqcSLURM.sh  \n",
    "```\n",
    "\n",
    "Transferred the multiqc html file to my local computer, renamed to \"_trimmed\". A handful of samples have low counts, but the adapter trimming looks good!\n",
    "\n",
    "Ran the alignment scripts one after the other:  \n",
    "`sbatch pcod-refs_alignARRAY.sh`  \n",
    "`sbatch scripts/pcod-refs_depthsARRAY.sh` - I had to modify this script to include the full path for the python script  `/home/lspencer/lcWGS-pipeline/mean_cov_ind.py`.  \n",
    "\n",
    "Created a barplot with mean coverage depth for each sample (sorted by depth). The lcWGS pipeline states that samples with mean depth much <1 should be blacklisted. 22 samples have depth <0.75, which I added to the blacklist file, \"blacklist.txt\". \n",
    "\n",
    "Ran `/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step4-data.py -p pcod-refs.ckpt -b blacklist.txt`, then ran the two resulting SLURM scripts: `sbatch scripts/pcod-refs_globalARRAY.sh` and `sbatch scripts/pcod-refs_polymorphicARRAY.sh`\n",
    "\n",
    "Ran the collate script: `/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step5-collate.py -p pcod-refs.ckpt`  \n",
    "Then ran the resulting SLURM scripts: `sbatch scripts/pcod-refs_concatenate_beagles.sh` & `sbatch scripts/pcod-refs_concatenate_mafs.sh`\n",
    "\n",
    "**Re-run AGNSD following WGSAssign's filtering approach**\n",
    "\n",
    "I re-ran ANGSD to use different filtering thresholds as per the WGSAssign [paper](https://github.com/mgdesaix/amre-adaptation/tree/main/02_PopulationGenetics)/[pipeline](https://github.com/mgdesaix/WGSassign). That script is called pcod-refs_wgassign-polymorphicARRAY.sh. \n",
    "\n",
    "#### Check reference SNPs for linkage disequilibrium \n",
    "\n",
    "Now I am abandoning the lcWGS pipeline, and following the protocol outlined here [mgdesaix/amre-adaptation](https://github.com/mgdesaix/amre-adaptation/tree/main/02_PopulationGenetics#subset-beagle-files) to check my list of SNPs in reference fish for linkage disequilibrium using the program `ngsLD`. Linked sites are typically pruned (removed) since their presence can bias results. As per the protocol, I referenced [this tutorial](https://github.com/nt246/lcwgs-guide-tutorial/blob/main/tutorial3_ld_popstructure/markdowns/ld.md#prepare-the-input-files) to prepare my input files for `ngsLD` (note: ngsLD is already on Sedna, which is handy).  \n",
    "\n",
    "##### Prep files for ngsLD program \n",
    "\n",
    "Prep `-geno` file: remove the first three columns (i.e. positions, major allele, minor allele) and header row from whole genome beagle file \n",
    "```\n",
    "zcat gls_wgassign/pcod-refs_wholegenome_wgassign.beagle.gz | cut -f 4- | tail -n +2 | gzip > gls_wgassign/ngsLD/pcod-refs_wholegenome_wgassign_4ngsLD.beagle.gz\n",
    "```\n",
    "\n",
    "Prep `-pos` file: input file with site coordinates (one per line), where the 1st column stands for the chromosome/contig and the 2nd for the position (bp). One convenient way to generate this is by selecting the first two columns of the mafs file outputted by ANGSD, with the header removed.\n",
    "```\n",
    "zcat gls_wgassign/pcod-refs_wholegenome_wgassign.mafs.gz | cut -f 1,2 | sed 's/:/_/g'| gzip > gls_wgassign/ngsLD/pcod-refs_wholegenome_wgassign_4ngsLD.sites.gz\n",
    "```\n",
    "\n",
    "With the slurm script \"pcod-refs_ngsLD.sh\" I ran the ngsLD program. `ngsLD` outputs a TSV file with LD results for all pairs of sites for which LD was calculated, where the first two columns are positions of the SNPs, the third column is the distance (in bp) between the SNPs, and the following 4 columns are the various measures of LD calculated. The amre-adaptation protocol prunes/removes correlated pairs (r > 0.5 in 7th column of ngsLD output file) with the program [`prune_graph`](), which Giles installed as a module on Sedna for me! I pruned linked SNPs using the script `referencde/scripts/pcod-refs_get-LDpruned.sh`. \n",
    "\n",
    "```\n",
    "(base) [lspencer@sedna ngsLD]$ cat ../../job_outfiles/LD-prune.txt\n",
    "[2024-07-22 12:10:22] INFO: Creating graph...\n",
    "[2024-07-22 12:10:22] INFO: Reading from input file...\n",
    "[2024-07-22 12:10:57] INFO: Input file has 431683 nodes with 20544101 edges (334507 edges with column_7 >= 0.5)\n",
    "[2024-07-22 12:10:57] INFO: Pruning heaviest position...\n",
    "[2024-07-22 12:12:06] INFO: Pruned 683 nodes in 68s (9.91 nodes/s); 431000 nodes remaining with 310916 edges.\n",
    "...\n",
    "[2024-07-22 14:42:15] INFO: Pruned 1000 nodes in 45s (22.22 nodes/s); 317000 nodes remaining with 339 edges.\n",
    "[2024-07-22 14:42:29] INFO: Pruning complete! Final graph has 316661 nodes with 0 edges\n",
    "[2024-07-22 14:42:29] INFO: Saving remaining nodes...\n",
    "[2024-07-22 14:42:30] INFO: Total runtime: 152.13 mins\n",
    "\n",
    "zcat pcod-refs_wholegenome_wgassign_4ngsLD.sites.gz | wc -l\n",
    "431691\n",
    "\n",
    "cat pcod-refs_wholegenome_unlinked | wc -l\n",
    "316661\n",
    "\n",
    "# Create sites file and index\n",
    "awk -F\":\" '{print $1, $2}' pcod-refs_wholegenome_unlinked > pcod-refs_wholegenome_unlinked.sites\n",
    "angsd sites index pcod-refs_wholegenome_unlinked.sites\n",
    "```\n",
    "\n",
    "#### Linkage disequilibrium pruning reduced the number of SNPs in my reference fish from 431,691 to 316,661 SNPs. Cool!  We will use this set of reference fish SNPs (referencde/gls_wgassign/ngsLD/pcod-refs_wholegenome_unlinked.sites) later when we run ANGSD on our experimental fish, so that we only analyze/retain the same SNP set! \n",
    "\n",
    "### Run lcWGS pipeline on Experimental fish\n",
    "\n",
    "Initiate interactiv node on Sedna\n",
    "`srun --pty /bin/bash`\n",
    "\n",
    "Activate virtual environment to access MultiQC: \n",
    "```\n",
    "source /home/lspencer/venv/bin/activate\n",
    "```\n",
    "\n",
    "Define path variables:\n",
    "```\n",
    "scripts=/home/lspencer/lcWGS-pipeline/\n",
    "inputs=/home/lspencer/pcod-lcwgs-2023/analysis-20240606/experimental/\n",
    "```\n",
    "\n",
    "Run first four python scripts, which generate slurm scripts needed for fastqc, trimming, and alignment: \n",
    "```\n",
    "${scripts}lcWGSpipeline_step0-configure.py -c ${inputs}lcWGS_config.txt\n",
    "${scripts}lcWGSpipeline_step1-qc.py -p pcod-exp.ckpt\n",
    "${scripts}lcWGSpipeline_step2-trim.py -p pcod-exp.ckpt\n",
    "/home/lspencer/lcWGS-pipeline/lcWGSpipeline_step3-align.py -p pcod-exp.ckpt\n",
    "```\n",
    "\n",
    "As per instructions, edited the MultiQC scripts (for raw and trimmed data) using nano to use the correct path to activate my virtual environment (/home/lspencer/venv/bin/activate)\n",
    "\n",
    "Then initiated these SLURM scripts (they ran at the same time):\n",
    "```\n",
    "sbatch scripts/GCF_031168955.1_ASM3116895v1_genomic_bwa-indexSLURM.sh\n",
    "sbatch scripts/GCF_031168955.1_ASM3116895v1_genomic_faiSLURM.sh\n",
    "sbatch scripts/pcod-lcWGS-raw_fastqcARRAY.sh\n",
    "```\n",
    "\n",
    "When the SLURM jobs were done, ran the multiqc script - sbatch scripts/pcod-exp-raw_multiqcSLURM.sh - After it finished I added \"_raw\" to file/director names, then transferred the multiqc files to my local computer to view.\n",
    "\n",
    "I then ran these scripts, **one after the other**, to trim and then look at the trimmed data:\n",
    "\n",
    "sbatch scripts/pcod-exp_trimARRAY.sh  \n",
    "sbatch scripts/pcod-exp-trim_fastqcARRAY.sh  \n",
    "sbatch scripts/pcod-exp-trim_multiqcSLURM.sh  \n",
    "\n",
    "I added \"_trimmed\" to file/director names, then transferred the multiqc html file to my local computer. Looks good!\n",
    "\n",
    "Ran the alignment scripts **one after the other**:\n",
    "\n",
    "sbatch pcod-exp_alignARRAY.sh\n",
    "sbatch scripts/pcod-exp_depthsARRAY.sh - I had to modify this script to include the full path for the python script /home/lspencer/lcWGS-pipeline/mean_cov_ind.py.\n",
    "\n",
    "I rsynced the pcod-exp_depths.csv file from sedna to local, read it into RStudio, and plotted depths by treatment. Mean depth across all samples is ~3x, which is great. No samples fall below the 1x depth threshold. There is a weird treatment difference, particularly in the 5C treatment. I asked Sam to contact the sequencing facility to see why that might be. \n",
    "\n",
    "Now, I am departing slightly from the AFSC lcWGS pipeline. \n",
    "\n",
    "I customized the ANGSD script to call and filter variants (polymorphic SNPs) only for sites identified as polymorphic/unlinked in the reference fish (using [`-sites <file>` angsd option](https://www.popgen.dk/angsd/index.php/Sites)) and which remained after linkage disequilibrium pruning. These are listed in the file /reference/gls_wgassign/ngsLD/pcod-refs_wholegenome_unlinked.sites, which has the format:  `chr pos`. NOTE: this sites file does NOT contain major and minor alleles (i.e. it is not an \"augmented\" sites file with 4 columns), but that's okay since we use the option `doMajorMinor 1` which infers major/minor alleles from genotype likelihoods. My custom ANGSD script is /experimental/scripts/pcod-exp_wgassign-polymorphicARRAY.s. \n",
    "\n",
    "After the custom angsd script ran, I concatenated the beagle.gz and mafs.gz files for separate chromosomes into wholegenome files (as per the AFSC lcWGS pipeline) using the scripts scripts/pcod-exp_concatenate_beagles.sh and scripts/pcod-exp_concatenate_mafs.s. How many polymorphic sites do I have for my experimental fish? 232,629. \n",
    "\n",
    "```\n",
    "cat gls_wgassign/pcod-exp_wholegenome_wgassign.sites | wc -l\n",
    "232,629\n",
    "``` \n",
    "\n",
    "#### Create beagle files containing the same SNP sites for both reference and experimental fish. \n",
    "\n",
    "I now have beagle files for both my reference and experimental fish. All sites in my experimental file are also in my reference fish dataset (since I restricted my experimental angsd analysis to only those sites). BUT, I still have additional sites in my _reference_ dataset. I want exactly the same sites for both. So ...  \n",
    "\n",
    "I subseted my _reference_ beagle file to only contain SNPs I identified in my experimental fish. This is so that I have 2 beagle files with the same sites. To do so I used code from the [mgdesaix/amre-adaptation github](https://github.com/mgdesaix/amre-adaptation/tree/main/02_PopulationGenetics#subset-beagle-files) (maker of WGSassign). \n",
    "\n",
    "First, generated a file with one column containing \"chromosome_location\" for all SNPs in the experimental fish: \n",
    "```\n",
    "gzip -cd gls_wgassign/pcod-exp_wholegenome_wgassign.beagle.gz | cut -f 1 > gls_wgassign/pcod-exp_wholegenome_wgassign.sites.simple\n",
    "```\n",
    "\n",
    "Then, I used the beagle filtering code: \n",
    "\n",
    "```\n",
    "ld_snps=gls_wgassign/pcod-exp_wholegenome_wgassign.sites.simple\n",
    "input=../reference/gls_wgassign/pcod-refs_wholegenome_wgassign\n",
    "output=../reference/gls_wgassign/pcod-refs_wholegenome_wgassign_filtered\n",
    "awk 'NR==FNR{c[$1]++;next};c[$1]' ${ld_snps} <(zcat ${input}.beagle.gz) | gzip > ${output}.beagle.gz\n",
    "```\n",
    "\n",
    "I checked how many loci are in the reference fish filtered SNP set and the experimental fish SNP set: \n",
    "```\n",
    "gzip -cd ../reference/gls_wgassign/pcod-refs_wholegenome_wgassign_filtered.beagle.gz | wc -l  #= 232,630\n",
    "gzip -cd gls_wgassign/pcod-exp_wholegenome_wgassign.beagle.gz | wc -l #= 232,630\n",
    "```\n",
    "\n",
    "Same number! To double check that I have the same loci, I compare the first columns of both files (thanks chat gpt):\n",
    "```\n",
    "diff <(zcat experimental/gls_wgassign/pcod-exp_wholegenome_wgassign.beagle.gz | awk '{print $1}') <(zcat reference/gls_wgassign/pcod-refs_wholegenome_wgassign_filtered.beagle.gz | awk '{print $1}')\n",
    "```\n",
    "\n",
    "### Here are the final 2 beagle files that contain genotype likelihoods for the same sites.  \n",
    "_Reference fish_:  /reference/gls_wgassign/pcod-refs_wholegenome_wgassign_filtered.beagle.gz  \n",
    "_Experimental fish_: /experimental/gls_wgassign/pcod-exp_wholegenome_wgassign.beagle.gz  \n",
    "\n",
    "\n",
    "\n",
    "### Explore population structure in reference fish \n",
    "\n",
    "Used `pcangsd` to generate covariance matrix from genome-wide genotype likelihoods with script reference/scripts/pcod-refs_wholegenome_pcangsd.sh. \n",
    "\n",
    "\n",
    "### Create one beagle file with both reference fish AND experimental fish\n",
    "I want to use PCA to look for overlaps among our reference fish and experimental fish. So, I need to merge the two beagle files. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090de96",
   "metadata": {},
   "source": [
    "## Pulling ZP3 haplotypes from lcGWS data\n",
    "\n",
    "I'm interested in the genetic differences among our juvenile Pacific cod specifically on the ZP3 gene. So, here I pull haplotypes (bp consensus sequences) for each fish for only the ZP3 gene region. \n",
    "\n",
    "First, subset .bam alignment files for only the ZP3 gene. I'm using .bam files from the \"analysis-20230828\" since that only contains our fish. \n",
    "\n",
    "```\n",
    "cd /home/lspencer/pcod-lcwgs-2023/analysis-20230828/bamtools/\n",
    "SAMPLES=$(ls *_sorted_dedup_clipped.bam | sed -e 's/pcod-lcWGS_//' | sed -e 's/_sorted_dedup_clipped.bam//')\n",
    "for sample in ${SAMPLES}\n",
    "do\n",
    "samtools view -b -h pcod-lcWGS_${sample}_sorted_dedup_clipped.bam \"Chr9:1867790-1875256\" > zp3-bams/${sample}.zp3.bam\n",
    "samtools sort zp3-bams/${sample}.zp3.bam > zp3-bams/${sample}.zp3.sorted.bam\n",
    "samtools index zp3-bams/${sample}.zp3.sorted.bam\n",
    "done\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
